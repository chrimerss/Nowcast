{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from unet import UNet\n",
    "from model import  Nowcast\n",
    "from GRU import RadarNet\n",
    "from dataloader import DataSet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from loss import ComLoss\n",
    "from dataloader import get_iter_dali\n",
    "import time\n",
    "from unet import UNet\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc= DoubleConv(4,64) #input (bsize,4,640,800)\n",
    "        self.down1= DownSample(64,128)\n",
    "        self.down2= DownSample(128,256)\n",
    "        self.down3= DownSample(256,512)\n",
    "        self.down4= DownSample(512, 1024)\n",
    "        \n",
    "        self.up1= UpSample(1536, 512)\n",
    "        self.up2= UpSample(768, 256)\n",
    "        self.up3= UpSample(384, 128)\n",
    "        self.up4= UpSample(384,64)\n",
    "        self.outConv= OutConv(128,2)\n",
    "        self.regout= nn.Conv2d(2,1,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1= self.inc(x)   #(bsize,64,640,800)\n",
    "        x2=self.down1(x1) #(bsize,128,320,400)\n",
    "        x3=self.down2(x2) #(bsize,256,160,200)\n",
    "        x4=self.down3(x3) #(bsize,512,80,100)\n",
    "        x4= nn.Dropout(0.5)(x4)\n",
    "        x5=self.down4(x4) #(bsize,1024,40,50)\n",
    "        x5= nn.Dropout(0.5)(x5)\n",
    "\n",
    "        out=self.up1(x5, x4) #(bsize,1536,80,100)\n",
    "        out=self.up2(out, x3)# (bsize,768,160,200)\n",
    "        out=self.up3(out, x2)# (bsize,384,320,400)\n",
    "        out= self.up4(out,x1)\n",
    "        out= self.outConv(out)# (bsize,1,50,50)\n",
    "        out=self.regout(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.doubleConv= nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, 3,1,1),\n",
    "                        nn.ReLU(True),\n",
    "                        nn.Conv2d(out_channels, out_channels, 3,1,1),\n",
    "                        nn.ReLU(True),\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        out= self.doubleConv(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv= nn.Sequential(\n",
    "                        \n",
    "                        nn.MaxPool2d(2),\n",
    "                        DoubleConv(in_channels, out_channels)\n",
    "                        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "    \n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv= DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1= self.up(x1)\n",
    "        diffY = x2.size()[2]-x1.size()[2]\n",
    "        diffX = x2.size()[3]-x1.size()[3]\n",
    "        \n",
    "        x1= F.pad(x1, [diffX//2, diffX-diffX//2,\n",
    "                      diffY//2, diffY-diffY//2])\n",
    "        \n",
    "#         print(x1.size(), x2.size())\n",
    "        \n",
    "        x= torch.cat([x2, x1], dim=1)\n",
    "\n",
    "        return self.conv(x)\n",
    "    \n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv2d(in_channels, out_channels,3,1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params(net):\n",
    "\t\tnum_params= 0\n",
    "\t\tfor param in net.parameters():\n",
    "\t\t\t\tnum_params+= param.numel()\n",
    "\n",
    "\t\tprint('Total number of parameters: %d'%num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (inc): DoubleConv(\n",
      "    (doubleConv): Sequential(\n",
      "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down1): DownSample(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (doubleConv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down2): DownSample(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (doubleConv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down3): DownSample(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (doubleConv): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down4): DownSample(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (doubleConv): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up1): UpSample(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (doubleConv): Sequential(\n",
      "        (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2): UpSample(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (doubleConv): Sequential(\n",
      "        (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up3): UpSample(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (doubleConv): Sequential(\n",
      "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up4): UpSample(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (doubleConv): Sequential(\n",
      "        (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outConv): OutConv(\n",
      "    (conv): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (regout): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Total number of parameters: 31492357\n",
      "learning rate 0.001000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 7.93 GiB total capacity; 6.00 GiB already allocated; 93.00 MiB free; 6.47 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20c6985b9924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mout_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programs/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1a207f19c461>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (bsize,768,160,200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (bsize,384,320,400)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (bsize,1,50,50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programs/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1a207f19c461>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         x1= F.pad(x1, [diffX//2, diffX-diffX//2,\n\u001b[0;32m---> 94\u001b[0;31m                       diffY//2, diffY-diffY//2])\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#         print(x1.size(), x2.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programs/miniconda3/envs/dl/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   3389\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding length too large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_pad_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding mode \"{}\"\" doesn\\'t take in value argument'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 7.93 GiB total capacity; 6.00 GiB already allocated; 93.00 MiB free; 6.47 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    classname= m.__class__.__name__\n",
    "    \n",
    "    if classname.find('conv2d')!=-1:\n",
    "        m.weight.data.uniform(0.0,1.0)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "use_gpu=True\n",
    "EVENTS= ['20170604', '20170624', '20170709','20170922','20180704','20181207','20190507','20190522','20190823','20190919']\n",
    "\n",
    "def num_params(net):\n",
    "\t\tnum_params= 0\n",
    "\t\tfor param in net.parameters():\n",
    "\t\t\t\tnum_params+= param.numel()\n",
    "\n",
    "\t\tprint('Total number of parameters: %d'%num_params)\n",
    "\n",
    "def normalizer(x):\n",
    "\t'''input tensor size (b,tsize,c,m,n), Apply log transform to data\n",
    "\t\tSee Casper et al. 2020 MetNet\n",
    "\t'''\n",
    "\tlog_transform= torch.log10(x+0.01)/4\n",
    "\ttangent_transform= torch.tanh(log_transform)\n",
    "\n",
    "\treturn tangent_transform \n",
    "\n",
    "def denormalizer(x):\n",
    "\t'''An inverse of normalizer'''\n",
    "\n",
    "\treturn torch.exp(x*4)-0.01\n",
    "\n",
    "\n",
    "\n",
    "# set up some parameters\n",
    "batch_size=4\n",
    "lr= 1e-3\n",
    "logging_path= 'logging/'\n",
    "num_epoches= 500\n",
    "epoch_to_save= 10\n",
    "\n",
    "\n",
    "# print(\"# of training samples: %d\\n\" %int(len(dataset_train)))\n",
    "\n",
    "model= UNet()\n",
    "# wandb.watch(model)\n",
    "# model=RadarNet(hidden_layers=16,use_gpu=True, device=0)\n",
    "# best_model_pth= '/home/allen/Documents/Projects/Nowcast/logging/net_epoch991.pth'\n",
    "# model.load_state_dict((torch.load(best_model_pth, map_location='cpu')))\n",
    "print(model)\n",
    "num_params(model)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# criterion= ComLoss()\n",
    "criterion= torch.nn.MSELoss()\n",
    "\n",
    "# model.load_state_dict(torch.load('../logging/newest-5_8.pth'))\n",
    "\n",
    "if use_gpu:\n",
    "    model= model.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "#optimizer\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler= MultiStepLR(optimizer, milestones=[100,400, 800], gamma=0.2)\n",
    "\n",
    "#start training\n",
    "model.train()\n",
    "step= 0\n",
    "for epoch in range(num_epoches):\n",
    "    start= time.time()\n",
    "\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('learning rate %f' %param_group['lr'])\n",
    "\n",
    "    for e, event in enumerate(EVENTS[:1]):\n",
    "        # ====================normal===============#\n",
    "        dataset_train= DataSet(event=event)\n",
    "        loader_train= DataLoader(dataset= dataset_train, num_workers=8, batch_size=batch_size, shuffle=True)\n",
    "        # ====================DALI===============#\n",
    "        # loader_train = get_iter_dali(event=EVENTS[0], batch_size=2,\n",
    "                                    # num_threads=8)\n",
    "        for i, data in enumerate(loader_train):\n",
    "            # input size: (4,10,1,200,200)\n",
    "            # target size: (4,10,1,200,200)\n",
    "            # ====================normal===============#\n",
    "            # input_train=Variable(torch.rand(size=(1,10,1,200,200)))\n",
    "            # target_train=Variable(torch.ones(size=(1,10,1,200,200)))\n",
    "            input_train=data[0].squeeze(axis=2)\n",
    "            target_train=data[1].squeeze(axis=2)\n",
    "            # ====================DALI===============#\n",
    "            # data= data[0]\n",
    "            # input_train=data['inputs']\n",
    "            # target_train=data['target']\n",
    "            optimizer.zero_grad()\n",
    "            # if model.radarnet.predictor.Who.weight.grad is not None:\n",
    "            # \tprint('before backward gradient: ', model.radarnet.predictor.Who.weight.grad.max())\n",
    "\n",
    "            # model.zero_grad()\n",
    "\n",
    "\n",
    "            input_train= normalizer(input_train)\n",
    "            # target_train= normalizer(target_train)\n",
    "            input_train, target_train= Variable(input_train), Variable(target_train)\n",
    "            if use_gpu:\n",
    "                input_train, target_train= input_train.cuda(), target_train.cuda()\n",
    "\n",
    "            out_train= model(input_train)\n",
    "            loss= criterion(target_train, out_train)\n",
    "\n",
    "            loss.backward()\n",
    "            # if model.radarnet.predictor.Who.weight.grad is not None:\n",
    "            # \tprint('after backward gradient: ', model.radarnet.predictor.Who.weight.grad.max())\n",
    "                # print('gradient: ', model.predictor.U_z.weight.grad.max())\n",
    "                # print('gradient: ', model.predictor.W_r.weight.grad.max())\n",
    "                # print('gradient: ', model.predictor.U_r.weight.grad.max())\n",
    "                # print('gradient: ', model.predictor.W_c.weight.grad.max())\n",
    "                # print('gradient: ', model.predictor.U_c.weight.grad.max())\t\t\t\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # output_train= torch.clamp(out_train, 0, 1)\n",
    "            # ================NORMAL================ #\n",
    "            print(\"[epoch %d/%d][event %d/%d][step %d/%d]  obj: %.4f \"%(epoch+1,num_epoches,e, len(EVENTS), i+1,len(loader_train),loss.item()))\n",
    "#             print(\"[epoch %d/%d][step %d/%d]  obj: %.4f \"%(epoch+1,num_epoches,  i+1,len(loader_train),loss.item()))\n",
    "            # ================DALI================ #\n",
    "            # print(\"[epoch %d/%d][event %d/%d][step %d]  obj: %.4f \"%(epoch+1,num_epoches,e, len(EVENTS), i+1,-loss.item()))\n",
    "\n",
    "            # print(list(model.parameters()))\n",
    "#             if step% 50 == 0:\n",
    "#                 try:\n",
    "#                     wandb.log({\"loss\": loss.item(),\n",
    "#                           \"pred\": wandb.Image(out_train.squeeze().cpu().detach().numpy()[-1][-1]),\n",
    "#                           'true': wandb.Image(target_train.squeeze().cpu().detach().numpy()[-1][-1])})\n",
    "#                 except:\n",
    "#                     print('recording failed.')\n",
    "\n",
    "            step+=1\n",
    "\n",
    "#save model\n",
    "    if epoch % epoch_to_save==0:\n",
    "        torch.save(model.state_dict(), os.path.join(logging_path,'net_epoch%d.pth'%(epoch+1)))\n",
    "    end= time.time()\n",
    "    print('One epoch costs %.2f minutes!'%((end-start)/60.))\n",
    "\n",
    "    scheduler.step(epoch)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(logging_path,'newest.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
